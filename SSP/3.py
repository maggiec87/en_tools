from bs4 import BeautifulSoup
import json
import os
import re

def is_contains_chinese(text):
    """åˆ¤æ–­å­—ç¬¦ä¸²æ˜¯å¦åŒ…å«ä¸­æ–‡å­—ç¬¦"""
    return len(re.findall(r'[\u4e00-\u9fa5]', text)) > 0

def extract_ssp_data(html_path, js_path):
    if not os.path.exists(html_path):
        print(f"âŒ æ‰¾ä¸åˆ°æ–‡ä»¶: {html_path}")
        return

    with open(html_path, 'r', encoding='utf-8') as f:
        soup = BeautifulSoup(f, 'html.parser')

    structured_data = {}
    # æŸ¥æ‰¾æ‰€æœ‰å†…å®¹è¡Œ
    rows = soup.find_all(class_='row')
    
    print(f"ğŸš€ æ­£åœ¨åˆ†æ HTML ç»“æ„å¹¶æå–æ•°æ®...")

    for row in rows:
        # 1. å¯»æ‰¾æ‰€å± Topic (å‘ä¸Šå¯»æ‰¾æœ€è¿‘çš„ h2)
        h2_tag = row.find_previous('h2')
        topic = h2_tag.get_text().strip() if h2_tag else "Default Topic"
        
        if topic not in structured_data:
            structured_data[topic] = []

        # 2. æå– content åŒºåŸŸå†…çš„æ‰€æœ‰æ–‡æœ¬ç¢ç‰‡
        content_div = row.find(class_='content')
        if not content_div:
            continue
            
        # ä½¿ç”¨è‡ªå®šä¹‰åˆ†éš”ç¬¦æå–æ‰€æœ‰å±‚çº§çš„æ–‡æœ¬ï¼Œé˜²æ­¢å•è¯ç²˜è¿
        all_text_parts = content_div.get_text(separator="|||", strip=True).split("|||")
        
        en_fragments = []
        zh_fragments = []

        for part in all_text_parts:
            # æ¸…ç†æ²‰æµ¸å¼ç¿»è¯‘å¯èƒ½å¼•å…¥çš„ç‰¹æ®Šå­—ç¬¦
            clean_part = part.replace('\n', ' ').strip()
            if not clean_part: continue
            
            # æ ¸å¿ƒåˆ¤å®šï¼šå«ä¸­æ–‡åˆ™å½’å…¥ä¸­æ–‡ç»„ï¼Œå¦åˆ™å½’å…¥è‹±æ–‡ç»„
            if is_contains_chinese(clean_part):
                zh_fragments.append(clean_part)
            else:
                # æ’é™¤å•çº¯çš„æ•°å­—åºå·ï¼ˆå¦‚ "1."ï¼‰
                if not re.match(r'^\d+\.$', clean_part):
                    en_fragments.append(clean_part)

        # 3. åˆå¹¶ç¢ç‰‡
        full_en = " ".join(en_fragments).replace("  ", " ").strip()
        full_zh = "".join(zh_fragments).strip()

        if full_en:
            structured_data[topic].append({
                "english": full_en,
                "chinese": full_zh
            })

    # 4. å†™å…¥ data.js ä¾›ç»ƒä¹ å†Œä½¿ç”¨
    with open(js_path, 'w', encoding='utf-8') as f:
        json_content = json.dumps(structured_data, ensure_ascii=False, indent=2)
        f.write(f"const sspData = {json_content};")
    
    print(f"âœ… å¤„ç†å®Œæˆï¼")
    print(f"ğŸ“Š ç»Ÿè®¡ï¼šå…±æå– {len(structured_data)} ä¸ªä¸»é¢˜ï¼Œæ•°æ®å·²ä¿å­˜è‡³ {js_path}")

# æ‰§è¡Œè½¬æ¢ï¼ˆè¯·ç¡®ä¿æ–‡ä»¶åä¸ä½ ä¸Šä¼ çš„ä¸€è‡´ï¼‰
extract_ssp_data("3. ssp_translated.html", "data.js")